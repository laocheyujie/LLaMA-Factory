## Docker 安装

```bash
docker pull hiyouga/llamafactory:latest
docker run -itd -v ./LLaMA-Factory:/app -v ./models:/models  --name llamafactory --gpus=all --ipc=host hiyouga/llamafactory:latest /bin/bash
```

```bash
# 安装基础工具
apt install -y zsh
wget https://gitee.com/mirrors/oh-my-zsh/raw/master/tools/install.sh
chmod +x install.sh
./install.sh
rm -rf ./install.sh
source ~/.zshrc
git clone https://gitee.com/cheyujie/zsh-autosuggestions.git ${ZSH_CUSTOM:-~/.oh-my-zsh/custom}/plugins/zsh-autosuggestions
git clone https://gitee.com/cheyujie/zsh-syntax-highlighting.git ${ZSH_CUSTOM:-~/.oh-my-zsh/custom}/plugins/zsh-syntax-highlighting
vi ~/.zshrc
# plugins=(git zsh-autosuggestions zsh-syntax-highlighting pip)
source ~/.zshrc
```

```bash
# 安装依赖包
pip config set global.index-url https://pypi.tuna.tsinghua.edu.cn/simple
pip config set global.extra-index-url https://pypi.tuna.tsinghua.edu.cn/simple

pip install --upgrade pip
pip install bitsandbytes vllm
pip install --no-deps ring-flash-attn
cd /workspace
pip install ./flash_attn-2.7.4.post1+cu12torch2.6cxx11abiFALSE-cp311-cp311-linux_x86_64.whl
pip install tensorboard nvitop yunchang
```

```bash
# 训练
llamafactory-cli train /app/examples/train_lora/qwen3_lora_sft_ds2.yaml
```


## 多机多卡训练

### 安装依赖
```bash
sudo apt-get update -y && sudo apt-get install -y pdsh ninja-build
```

### 查看网络信息
```bash
ip link show

# 查看网卡速率 enttool
# Speed: 10000Mb/s 表示 10Gbe 万兆
sudo ethtool ens21f0 | grep Speed

# 查看网卡速率 ib
# ibstat
```

### NCCL 配置
```bash
export NCCL_SOCKET_IFNAME=ens21f0
```

### SSH 配置
```bash
# cat /etc/ssh/sshd_config | grep PermitRootLogin
# 如果不是 yes
# sed -i 's/^.*PermitRootLogin.*$/PermitRootLogin yes/g' /etc/ssh/sshd_config
vi ~/.ssh/config
```

```bash
Host worker_1
        User  username
        Hostname 192.168.1.100
        # port 2223
        IdentityFile ~/.ssh/id_rsa
Host worker_2
        User  username
        Hostname 192.168.1.101
        # port 2223
        IdentityFile ~/.ssh/id_rsa  
```

```bash
ssh-copy-id worker_1
ssh-copy-id worker_2
```

### Hostfile 配置
```bash
vi hostfile
```

```bash
worker_1 slots=8
worker_2 slots=8
```

### Python 环境
```bash
conda create -n llamafactory python=3.11
conda activate llamafactory
git clone git@github.com:hiyouga/LLaMA-Factory.git
cd LLaMA-Factory
pip install -e ".[torch,metrics,deepspeed,bitsandbytes,vllm,swanlab]" --no-build-isolation
pip install wandb nvitop
```

Install FlashAttention:
https://github.com/Dao-AILab/flash-attention/releases/expanded_assets/v2.8.1 下载 flash_attn

```bash
pip install ./xxx.whl
```

### 训练环境
```bash
vi ~/.zshrc
export WANDB_API_KEY=xxx
export SWANLAB_API_KEY=xxx
source ~/.zshrc
```

### 训练数据
```bash
cd data
cp /data/cheyujie/datasets/data.json ./
vi dataset_info.json
```


### DeepSpeed 方式启动
```bash
deepspeed --num_gpus 8 --num_nodes 2 --hostfile hostfile --master_addr 10.1.50.7 --master_port=9901 \
/data1/cheyujie/code/LLaMA-Factory/src/train.py \
--model_name_or_path /data1/cheyujie/models/ZhipuAI/GLM-Z1-32B-0414 \
--trust_remote_code true \
--flash_attn fa2 \
--stage pt \
--template glmz1 \
--do_train true \
--finetuning_type lora \
--lora_rank 16 \
--lora_alpha 16 \
--lora_dropout 0.0 \
--lora_target all \
--deepspeed /data1/cheyujie/code/LLaMA-Factory/examples/deepspeed/ds_z2_config.json \
--dataset gf \
--cutoff_len 4096 \
--overwrite_cache true \
--preprocessing_num_workers 64 \
--dataloader_num_workers 16 \
--output_dir /data1/cheyujie/code/LLaMA-Factory/saves/glm-z1-32b-gf/lora/pretrain \
--save_on_each_node true \
# --resume_from_checkpoint /data1/cheyujie/code/LLaMA-Factory/saves/glm-z1-32b-gf/lora/pretrain/checkpoint-2184 \
--logging_steps 2 \
--save_steps 200 \
--plot_loss true \
--overwrite_output_dir true \
--save_only_model false \
--report_to swanlab \
--run_name GLM-Z1-32B-GF \
--use_swanlab true \
--swanlab_api_key lTmfVtTliz32hDQ2FNDG9 \
--swanlab_run_name GLM-Z1-32B-GF \
--per_device_train_batch_size 1 \
--gradient_accumulation_steps 4 \
--learning_rate 1.0e-4 \
--optim adamw_torch \
--max_grad_norm 1.0 \
--num_train_epochs 16 \
--lr_scheduler_type cosine \
--warmup_ratio 0.0 \
--bf16 true \
--ddp_timeout 180000000
```

> 如果启用了`resume_from_checkpoint`，则需要修改`output_dir`位置


### Accelerate 方式启动
```bash
accelerate launch \
--config_file examples/accelerate/deepspeed_config.yaml \
src/train.py examples/train_lora/glm_z1_lora_pretrain_gf.yaml
```

```bash
accelerate launch \
--config_file examples/accelerate/fsdp_config_2.yaml \
src/train.py examples/train_lora/glm_z1_lora_pretrain_gf.yaml
```