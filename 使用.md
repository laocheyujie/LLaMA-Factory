## Debug
在 `LLaMA-Factory/src/llamafactory/launcher.py` 页面进行 debug


## 更新模型
1. 新增模型仓库: 去 `src/llamafactory/extras/constants.py` 里 `register_model_group` 注册模型仓库信息
3. **新增 Template**: 去 `src/llamafactory/data/template.py` 里 `register_template` 注册模板
4. 新增 Tool: 去 `src/llamafactory/data/tool_utils.py` 里新增 TOOL_PROMPT 和 ToolUtils 类，并加入 `TOOLS` 字典
4. 新增多模态配置: 去 `src/llamafactory/model/model_utils/visual.py` 里 `_register_composite_model` 添加映射
5. 设置 DeepSpeed ZeRO-3 MoE 配置: 去 `src/llamafactory/model/model_utils/moe.py` 里 `_set_z3_leaf_modules` 为新增模型的 transformers 模型架构


## Docker 安装

```bash
docker pull hiyouga/llamafactory:latest
docker run -itd -v ./LLaMA-Factory:/app -v ./models:/models  --name llamafactory --gpus=all --ipc=host hiyouga/llamafactory:latest /bin/bash
```

```bash
# 安装基础工具
apt install -y zsh
wget https://gitee.com/mirrors/oh-my-zsh/raw/master/tools/install.sh
chmod +x install.sh
./install.sh
rm -rf ./install.sh
source ~/.zshrc
git clone https://gitee.com/cheyujie/zsh-autosuggestions.git ${ZSH_CUSTOM:-~/.oh-my-zsh/custom}/plugins/zsh-autosuggestions
git clone https://gitee.com/cheyujie/zsh-syntax-highlighting.git ${ZSH_CUSTOM:-~/.oh-my-zsh/custom}/plugins/zsh-syntax-highlighting
vi ~/.zshrc
# plugins=(git zsh-autosuggestions zsh-syntax-highlighting pip)
source ~/.zshrc
```

```bash
# 安装依赖包
pip config set global.index-url https://pypi.tuna.tsinghua.edu.cn/simple
pip config set global.extra-index-url https://pypi.tuna.tsinghua.edu.cn/simple

pip install --upgrade pip
pip install bitsandbytes vllm
pip install --no-deps ring-flash-attn
cd /workspace
pip install ./flash_attn-2.7.4.post1+cu12torch2.6cxx11abiFALSE-cp311-cp311-linux_x86_64.whl
pip install tensorboard nvitop yunchang
```

```bash
# 训练
llamafactory-cli train /app/examples/train_lora/qwen3_lora_sft_ds2.yaml
```


## 多机多卡训练

### 安装依赖
```bash
sudo apt-get update -y && sudo apt-get install -y pdsh ninja-build
```

### 查看网络信息
```bash
ip link show

# 查看网卡速率 enttool
# Speed: 10000Mb/s 表示 10Gbe 万兆
sudo ethtool ens21f0 | grep Speed

# 查看网卡速率 ib
# ibstat
```

### NCCL 配置
```bash
export NCCL_SOCKET_IFNAME=ens21f0
```

### SSH 配置
```bash
# cat /etc/ssh/sshd_config | grep PermitRootLogin
# 如果不是 yes
# sed -i 's/^.*PermitRootLogin.*$/PermitRootLogin yes/g' /etc/ssh/sshd_config
vi ~/.ssh/config
```

```bash
Host worker_1
        User  username
        Hostname 192.168.1.100
        # port 2223
        IdentityFile ~/.ssh/id_rsa
Host worker_2
        User  username
        Hostname 192.168.1.101
        # port 2223
        IdentityFile ~/.ssh/id_rsa  
```

```bash
ssh-copy-id worker_1
ssh-copy-id worker_2
```

### Hostfile 配置
```bash
vi hostfile
```

```bash
worker_1 slots=8
worker_2 slots=8
```

### Python 环境
```bash
conda create -n llamafactory python=3.11
conda activate llamafactory
git clone git@github.com:hiyouga/LLaMA-Factory.git
cd LLaMA-Factory
pip install -e ".[torch,metrics,deepspeed,bitsandbytes,vllm,swanlab]" --no-build-isolation
pip install wandb nvitop
```

Install FlashAttention:
https://github.com/Dao-AILab/flash-attention/releases/expanded_assets/v2.8.1 下载 flash_attn

```bash
pip install ./xxx.whl
```

### 训练环境
```bash
vi ~/.zshrc
export WANDB_API_KEY=xxx
export SWANLAB_API_KEY=xxx
source ~/.zshrc
```

### 训练数据
```bash
cd data
cp /data/cheyujie/datasets/data.json ./
vi dataset_info.json
```


### DeepSpeed 方式启动
```bash
deepspeed --num_gpus 8 --num_nodes 2 --hostfile hostfile --master_addr 10.1.50.7 --master_port=9901 \
/data1/cheyujie/code/LLaMA-Factory/src/train.py \
--model_name_or_path /data1/cheyujie/models/ZhipuAI/GLM-Z1-32B-0414 \
--trust_remote_code true \
--flash_attn fa2 \
--stage pt \
--template glmz1 \
--do_train true \
--finetuning_type lora \
--lora_rank 16 \
--lora_alpha 16 \
--lora_dropout 0.0 \
--lora_target all \
--deepspeed /data1/cheyujie/code/LLaMA-Factory/examples/deepspeed/ds_z2_config.json \
--dataset gf \
--cutoff_len 4096 \
--overwrite_cache true \
--preprocessing_num_workers 64 \
--dataloader_num_workers 16 \
--output_dir /data1/cheyujie/code/LLaMA-Factory/saves/glm-z1-32b-gf/lora/pretrain \
--save_on_each_node true \
# --resume_from_checkpoint /data1/cheyujie/code/LLaMA-Factory/saves/glm-z1-32b-gf/lora/pretrain/checkpoint-2184 \
--logging_steps 2 \
--save_steps 200 \
--plot_loss true \
--overwrite_output_dir true \
--save_only_model false \
--report_to swanlab \
--run_name GLM-Z1-32B-GF \
--use_swanlab true \
--swanlab_api_key lTmfVtTliz32hDQ2FNDG9 \
--swanlab_run_name GLM-Z1-32B-GF \
--per_device_train_batch_size 1 \
--gradient_accumulation_steps 4 \
--learning_rate 1.0e-4 \
--optim adamw_torch \
--max_grad_norm 1.0 \
--num_train_epochs 16 \
--lr_scheduler_type cosine \
--warmup_ratio 0.0 \
--bf16 true \
--ddp_timeout 180000000
```

**特别注意：**

多机多卡使用 `--save_on_each_node true` 就可以断点重训。
而如果一开始没有加这个参数，可以在将主机上的 checkpoint 文件夹复制到从机，再把从机单独保存的 bf16_zero_pp_rank_1_mp_rank_00_optim_states.pt 文件放到 global_step 文件夹下后，可以正常继续训练


> 如果启用了`resume_from_checkpoint`，则需要修改`output_dir`位置

> 参考：[LLaMA-Factory多机多卡训练](https://blog.csdn.net/xiong_wq/article/details/143189413)


### Accelerate 方式启动
```bash
accelerate launch \
--config_file examples/accelerate/deepspeed_config.yaml \
src/train.py examples/train_lora/glm_z1_lora_pretrain_gf.yaml
```

```bash
accelerate launch \
--config_file examples/accelerate/fsdp_config_2.yaml \
src/train.py examples/train_lora/glm_z1_lora_pretrain_gf.yaml
```

> 如果报错，把 yaml 的内容以命令行参数的形式输入



## 训练
### tool calling
只支持 ..., function_call, observation, ..., ..., function_call, observation, ... 按顺序成对出现